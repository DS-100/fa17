{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Compatible Primary Keys\n",
    "\n",
    "\n",
    "### Simple String Manipulation\n",
    "\n",
    "Manipulating strings directy, without using regular expressions, can get us a long way. As an example, we will develop some code to clean the county names, where the code does not use regular expressions. \n",
    "\n",
    "Let's create some sample data that has all of the issues that we want to fix in a few county names. Recall that we want to:\n",
    "\n",
    "\n",
    "1. Change all letters to lower case.\n",
    "1. Drop county and parish from the end of the string.\n",
    "1. Change '&' to 'and'\n",
    "1. Drop all periods\n",
    "1. Eliminate all spaces/blanks\n",
    "\n",
    "The following strings have all of these features that need fixing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "county_ex = [\n",
    "    'De witt County', \n",
    "    'Lac qui Parle County', \n",
    "    'St. John the Baptist Parish', \n",
    "    'Stone County', \n",
    "    'Lewis & Clark County'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we want to convert the letters to lower case first because then we know that we are only working with lower case.\n",
    "\n",
    "In our example, \"county\" and \"parish\" are both 6 letters long so we need not do anything fancy to eliminate this word from each string. We can simply drop the last 6 characters.\n",
    "\n",
    "Then we can go through the characters in each string, one at a time, and eliminate the blanks and periods and replace the \"&\" with \"and\".\n",
    "\n",
    "The code below takes care of these string manipulations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for s in county_ex:\n",
    "    lower = s.lower()\n",
    "    wo_county = lower[:-7]\n",
    "    def dropchange(l):\n",
    "        if l == \"&\":\n",
    "            return \"and\"\n",
    "        elif l == \" \" or l == \".\":\n",
    "            return \"\" \n",
    "        else:\n",
    "            return l\n",
    "    final_word = \"\".join([dropchange(l) for l in wo_county])\n",
    "    final.append(final_word)\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an alternative approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for s in county_ex:\n",
    "    lower = s.lower()\n",
    "    without_county = lower[:-7]\n",
    "    def amptoand(l):\n",
    "        if l == \"&\":\n",
    "            return \"and\"\n",
    "        else:\n",
    "            return l\n",
    "    final_word = \"\".join([amptoand(l) for l in without_county if l != \".\" and l != \" \"])\n",
    "    final.append(final_word)\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did we eliminate all blanks from the names, rather than eliminate from De Witt County? We looked at only a few records and can imagine that there are other cases where we want to drop a blank. Also, removing all of the blanks could make the county names easier to work with in general.  Is there a downside to dropping the spaces? If a county is no longer uniquely identified without the blanks in its name, then that could be a problem. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python list comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can instead use Python list comprehension to make the changes to `county_ex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_cty = [\n",
    "    w.lower()[:-7] # fancy indexing\n",
    "    .replace(\".\",\"\")\n",
    "    .replace(\"&\",\"and\")\n",
    "    .replace(\" \", \"\") \n",
    "    for w in county_ex\n",
    "]\n",
    "print(cleaned_cty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data frame\n",
    "\n",
    "We can also work with the county name as a column in a dataframe and clean it.\n",
    "Below are two approaches to create a data frame from `county_ex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.Series(county_ex, name=\"County\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = county_ex, columns=[\"County\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to return to the original format we can get the county strings back out with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['County']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python list comprehension\n",
    "\n",
    "Lastly, we use the string manipulation available for Pandas dataframes to perform the same cleaning. Rather than loop through each character in the string one at a time, we use `str` to work on the string. The method `replace` takes regular expressions as arguments. An example below is \"county|parish\" this pattern will match either the characters \"county\" or the characters \"parish\" and if `replace` finds a match anywhere in the string, it will replace it with \"\", i.e., it will drop it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df['County'].str.lower()\n",
    "    .str.replace(\"county\", \"\")\n",
    "    .str.replace(\"parish\", \"\")\n",
    "    .str.replace(\".\",\"\")\n",
    "    .str.replace(\"&\",\"and\")\n",
    "    .str.replace(\" \", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a more compact version that consolidates the replacements using metacharacters in regular expressions. For example, An example, the \"county|parish\" pattern matches either the characters \"county\" or the characters \"parish\" so if `replace` finds a match anywhere in the string, it will replace these 6 characters with \"\", i.e., they will be dropped. \n",
    "\n",
    "Also, the pattern \"[. ]\" says that either a period or a blank are to be treated equivalently because they are within the square brackets. Now, if either a blank or a period is found, then it will be replaced with \"\" (that is, it will be dropped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df['County'].str.lower()\n",
    "    .str.replace(\"county|parish\",\"\")\n",
    "    .str.replace(\"[. ]\",\"\")\n",
    "    .str.replace(\"&\",\"and\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have introduced some concepts related to pattern matching, such as | and [ ]. We will cover more in detail in the examples in class slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Logs\n",
    "\n",
    "In our next example, we process strings that have some structure, but not enough to always finr values between commas, say, or in fixed positions in all reacords. We can get a long way with simple string manipulation in this example too, but we will demonstrate how powerful a regular expression can be in extracting fields from strings.\n",
    "\n",
    "These data are logs from a web site. Again, we use a small data set to demonstrate how we might process the file, but we don't actually process and analyze the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lh data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample file has only two records as we see in the call to `head` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/smallLog.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to read each line of the file in as a string and then process the strings to create variables. We use `readlines` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/smallLog.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "print(len(lines))\n",
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can place `lines` in a dataframe and work with the strings as a column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.Series(lines, name=\"raw\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each log entry contains a lot of information such as the:\n",
    "\n",
    "* Page visited\n",
    "* Date and time of visit\n",
    "* Browser used\n",
    "* IP address \n",
    "\n",
    "When we examine the records, we see that there are certain characters that separate the fields. In our two sample records some of the pieces of information appear in the same location, but we don't want to rely on that always being the case. For example, the IP address at the beginning of the string need not always be 14 characters long. It might be 15 characters if all four of the numbers have 3 digits. This would then shift the rest of the information in the string. Instead, we want to key on the separators to locate the information.\n",
    "\n",
    "Sometimes these are dashes, other times they are square brackets, commas, or colons. Let's consider the task of extracting the day, month, and year each as a string. One way to do this is to split the string into pieces. We can do this with the `split` method. \n",
    "\n",
    "We first split the string many times over to zoom in on the substring(s) of interest. That is, the date appears between `[` and `]`\n",
    "\n",
    "Let's first split the string at the left square brackets. Notice that the square brackets are \"eaten\" by the splitting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = df['raw'].str.split(r'[').str\n",
    "date[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second substring contains the date. If we split the second substring at colons then the first of its substrings will contain the day, month, and year. That is, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date2 = date[1].str.split(r':').str[0]\n",
    "print(date2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['raw']\n",
    " .str.split(r'[').str[1]\n",
    " .str.split(r':').str[0]\n",
    " .str.split(r'/').str[0:3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date3 = date2.str.split(r'/').str[0:3]\n",
    "df['day'] = date3.str[0]\n",
    "df['mon'] = date3.str[1]\n",
    "df['Yr'] = date3.str[2]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can split this string, e.g., \"26/Jan/2014\", on the forward slash and assign each of the 3 resulting substrings to day, month, and year variables in `df1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform all of these splits at once with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_pieces = (df['raw']\n",
    "               .str.split(r'[').str[1]\n",
    "               .str.split(r':').str[0]\n",
    "               .str.split(r'/').str[0:3]\n",
    "              )\n",
    "print(date_pieces)\n",
    "df['day'] = date_pieces.str[0]\n",
    "df['mon'] = date_pieces.str[1]\n",
    "df['Yr'] = date_pieces.str[2]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However with regular expressions, we can split on any of the 3 characters, '[' or ':' or \"/\", in one call to `split`. That is, we can split the string on 3 equivalent and then select the desired substrings. We do this as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(pd.Series(lines, name=\"raw\"))\n",
    "date = df1['raw'].str.split(r'[\\[/:]').str[1:4]\n",
    "df1['day'] = date.str[0]\n",
    "df1['mon'] = date.str[1]\n",
    "df1['yr'] = date.str[2]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above call to `split` we passed a regular expression, '[\\\\[/:]'. This is a special pattern.  The outer brackets indicate that the characters within are to be treated equivalently. The literals are within the brackets; these are the left bracket, forward slash, and colon.  Equivalent characters means: \"when any of these are found\" split the string. Notice the backslash preceeding the left bracket withing the outer brackets. This is a special character that indicates that the [ is to be treated as the literal backslash, i.e., it is not a special character for the regex engine. We delve into regular expressions in greater depth in the next example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Food Safety\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vio = pd.read_csv(\"../../hw/hw2/data/violations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_date = vio['date'].apply(lambda d: pd.datetime.strptime(str(d),'%Y%m%d'))\n",
    "vio['year']  = new_date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vio = vio[vio['year'] == 2016].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vio['description'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vio['description'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vio['description'].value_counts().head(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vio['description'].value_counts().tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the [ date violation corrected ...] piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vio['desc'] = vio['description'].str.replace(\"\\[.*\\]\",\"\").str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check how many unique violation descriptions we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vio['desc'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vio['desc'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vio['desc'].value_counts().tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.get_dummies(vio['desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex = re.compile(r\"clean|sanitary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "vio['desc'].str.contains(regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "vio['desc'].str.contains(r\"clean|sanitary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive variables that indicate violations\n",
    "\n",
    "Let's create some variables that capture the essence of a type of violation. We identified violations related to:\n",
    "\n",
    "* vermin\n",
    "* risk\n",
    "* walls, ceilings, floors, and nonfood surfaces\n",
    "* food surfaces\n",
    "* hands, gloves, hair, nails\n",
    "\n",
    "And more.\n",
    "\n",
    "Let's begin by creating a variable related to cleanliness. The regular expression below matches on \"clean\" or on \"sanit\". Why do we use \"clean\" rather than unclean? Why do we use \"sanit\" rather than unsanitary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vio['isClean'] = vio['desc'].str.contains(r\"clean|sanit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any violation with \"clean\" in the description will be about a cleanliness violation. We do not need to search for \"unclean\" or \"not clean\" or some other description that indicates there is a violation related to being clean. The same goes for \"sanit\", it will find not sanitary, unsanitary, and unsanitized, etc.\n",
    "\n",
    "Next we look for violations that contain both \"surface\" and \"\\Wfood\". This means that there is a violation related to the food surface. Why do we use the meta character \"\\W\"? To avoid matching on words such as \"nonfood\" and so pick up issues with nonfood surfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "surface = vio['desc'].str.contains(r\"surface\")\n",
    "food = vio['desc'].str.contains(r\"\\Wfood\")\n",
    "vio['isFoodSurface'] = (surface & food)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create an ordinal variable that measures whether a violation is no risk (0), low risk (1), medium risk (2) or high risk (3). We first create 3 logical variables to to represent low, medium and high risk and then combine them to get an ordinal variable. Why is it ordinal and not quantitative?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vio['isHighRisk'] = vio['desc'].str.contains(r\"high risk\")\n",
    "vio['isMedRisk'] = vio['desc'].str.contains(r\"medium risk\")\n",
    "vio['isLowRisk'] = vio['desc'].str.contains(r\"low risk\")\n",
    "#create risk which is 1 2 3 for low medium or high\n",
    "vio[\"risk\"] = vio['isLowRisk'] + 2*vio['isMedRisk'] + 3*vio['isHighRisk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A few more variables for types of violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vio['isVermin'] = vio['desc'].str.contains(r\"vermin\")\n",
    "vio['isStorage'] = vio['desc'].str.contains(r\"thaw|cool|therm|storage\")\n",
    "vio['isDisplayPermit'] = vio['desc'].str.contains(r\"certificate|permit\")\n",
    "vio['isNonFoodSurface'] = vio['desc'].str.contains(r\"wall|ceiling|floor|nonfood surface\")\n",
    "vio['isHuman'] = vio['desc'].str.contains(r\"hand|glove|hair|nail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate by business and Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vio.groupby(['business_id', 'date']).aggregate({\n",
    "        \"isClean\": sum,\n",
    "        \"isFoodSurface\": sum,\n",
    "        \"isVermin\": sum,\n",
    "        \"isStorage\": sum,\n",
    "        \"isDisplayPermit\": sum,\n",
    "        \"isNonFoodSurface\": sum,\n",
    "        \"isHuman\": sum,\n",
    "        \"isHighRisk\": sum,\n",
    "        \"risk\": max\n",
    "    })\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = vio.groupby(['business_id', 'date'])[['isClean', 'isFoodSurface', 'isVermin',\n",
    "                                           'isStorage', 'isDisplayPermit', 'isHuman',\n",
    "                                          'isNonFoodSurface', 'isHighRisk',]].sum()\n",
    "b = vio.groupby(['business_id', 'date'])[['risk']].max()\n",
    "\n",
    "features = a.join(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vio.groupby(['business_id', 'date'])[['isClean', 'isFoodSurface', 'isVermin',\n",
    "                                                'isStorage', 'isDisplayPermit', 'isHuman',\n",
    "                                                'isNonFoodSurface', 'isHighRisk']].sum()\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = pd.read_csv(\"../../hw/hw2/data/inspections.csv\").set_index(['business_id', 'date'])\n",
    "ins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.join(features).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vio.groupby(['business_id', 'date'], as_index=False)[['isClean', 'isFoodSurface', \n",
    "                                                                 'isVermin', 'isStorage', \n",
    "                                                                 'isDisplayPermit', 'isHuman',\n",
    "                                                                 'isNonFoodSurface', 'isHighRisk']].sum()\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = pd.read_csv(\"../../hw/hw2/data/inspections.csv\")\n",
    "ins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data = pd.merge(features, ins, on=['business_id', 'date'])\n",
    "joined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['isClean', 'isHighRisk', 'isVermin', 'isHuman',\n",
    "                 'isStorage', 'isFoodSurface', 'isNonFoodSurface', 'isDisplayPermit']\n",
    "import sklearn.linear_model as lm\n",
    "model = lm.LinearRegression()\n",
    "model.fit(joined_data[feature_names], joined_data['score'])\n",
    "model.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "i = 1\n",
    "for f in feature_names:\n",
    "    plt.subplot(2, 4, i)\n",
    "    sns.boxplot(y=\"score\", x=f, data=joined_data)\n",
    "    i += 1\n",
    "#plt.savefig(\"violationBoxplts.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from these boxplots that not having a permit displayed has little impact on the score. Violations that involve the improper storage of food or the unsanitary handling of food by people and vermin reduce the score. Multiple violations reduce the score even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State of the union address\n",
    "\n",
    "We have the text from all of the state of the union addresses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/stateoftheunion1790-2017.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the speeches are separated by \\*\\*\\*. We can separate the speeches by splitting on 3 asterisks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "records = text.split(\"***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the \\*\\*\\*, a speech is formatted as\n",
    "```\n",
    "\n",
    "State of the Union Address\n",
    "George Washington\n",
    "January 8, 1790\n",
    "\n",
    "Fellow-Citizens of the Senate and House of Representatives:\n",
    "\n",
    "I embrace with great satisfaction the opportunity which now presents itself...\n",
    "```\n",
    "If we split on new lines, i.e., \\n then we can extract the name and date easily.\n",
    "The rest of the lines can be joined together into one string containing the text of the speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parts(line):\n",
    "    parts = line.split(\"\\n\")\n",
    "    name = parts[3].strip()\n",
    "    date = parts[4].strip()\n",
    "    text = \"\\n\".join(parts[5:]).strip()\n",
    "    return [name, date, text]\n",
    "\n",
    "df = pd.DataFrame([extract_parts(l) for l in records[1:]], columns=[\"Name\", \"Date\", \"Text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a few simple text cleaning tasks. We convert characters to lower case, eliminate the new lines, and drop all punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean text'] = (\n",
    "    df['Text']\n",
    "    .str.replace(\"\\n\", \" \")\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[^a-z\\s]\", \" \")\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use `sklearn` to create a word vector for each speech, which contains the counts of all words in a speech. By all words, we mean the set of all unique words used across all 226 speeches.  We can think of each word vector as a record so we have 226 records and thousands of variables (word counts). \n",
    "\n",
    "\n",
    "We can try to examine the relationship between speeches by reducing the dimensionality of the data.  We take an approach that is a kind of Principle Component Analysis for word vectors. Specifically, we measure the distance between speeches via a metric on the word vectors. This metric normalizes by the rarity of a word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer()\n",
    "tfidf = vec.fit_transform(df['clean text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a 226 by 226 matrix of the distances between all pairs of speeches. Then we use SVD to decompose the matrix and plot the first two column vectors of the resulting decomposition (these are similar in nature to the first two principle components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "import scipy as sp\n",
    "(u, s, vt) = sp.sparse.linalg.svds(tfidf, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Year'] = df['Date'].str[-4:].astype('int')\n",
    "df['x'] = u[:,0]\n",
    "df['y'] = u[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x = 'x', y = 'y', data = df, hue='Year', legend=False, \n",
    "           fit_reg=False, palette=\"Blues\")\n",
    "plt.savefig(\"SOTUspeeches.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array([\"rgba({0},{1},{2},1)\".format(*c) for c in sns.color_palette(\"Blues\", len(df))])\n",
    "colors[-1] = \"rgba(.99,.5,.2,1.)\"\n",
    "py.iplot([go.Scatter(x = df['x'], y = df['y'], mode='markers', marker=dict(color=colors), text=df['Name'])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Each point represents a speech. Notice that the speeches are roughly align chronologically. Speeches long ago are more similar to one another than current speeches. The most unusual speech is by Herbert Hoover. George Bush also has a few unusual speeches. Trumps speech is close to ealier speeches by Ronald Reagan and George Bush and Bill Clinton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
